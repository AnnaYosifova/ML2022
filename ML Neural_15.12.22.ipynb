{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c36f0862",
   "metadata": {},
   "source": [
    "## Создание искусственных нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b990e34d",
   "metadata": {},
   "source": [
    "### Импорт собранных и обработанных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daeca458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_rubric</th>\n",
       "      <th>rubric</th>\n",
       "      <th>text_lemm</th>\n",
       "      <th>title_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Происшествия</td>\n",
       "      <td>Происшествия</td>\n",
       "      <td>следствие попадать глава аналитический отдел п...</td>\n",
       "      <td>обвинение взяточничество арестовывать подмоско...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Происшествия</td>\n",
       "      <td>Происшествия</td>\n",
       "      <td>систематическое получение взятка арестовывать ...</td>\n",
       "      <td>систематическое получение взятка арестовывать ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Происшествия</td>\n",
       "      <td>Происшествия</td>\n",
       "      <td>место выезжать аварийный бригада сотрудник адм...</td>\n",
       "      <td>приграничное село белгородский область попадат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Происшествия</td>\n",
       "      <td>Происшествия</td>\n",
       "      <td>село старый хутор белгородский область попадат...</td>\n",
       "      <td>украинские войско обстреливать село белгородск...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Происшествия</td>\n",
       "      <td>Происшествия</td>\n",
       "      <td>четверг декабрь обстрел вооруженный формирован...</td>\n",
       "      <td>губернатор гладков всу обстреливать село стары...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3391</th>\n",
       "      <td>Наука</td>\n",
       "      <td>Космос</td>\n",
       "      <td>ученые томский университет система управление ...</td>\n",
       "      <td>томские ученый разрабатывать система электропи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3392</th>\n",
       "      <td>Наука</td>\n",
       "      <td>Космос</td>\n",
       "      <td>вес бортовой система космический аппарат число...</td>\n",
       "      <td>томске разрабатывать система электропитание сп...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3393</th>\n",
       "      <td>Наука</td>\n",
       "      <td>Космос</td>\n",
       "      <td>двадцать пятый август год зарегистрировать мар...</td>\n",
       "      <td>ученые зафиксировать марсотрясение рекордный силы</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3394</th>\n",
       "      <td>Наука</td>\n",
       "      <td>Космос</td>\n",
       "      <td>сейсмометр марсианский модуль insight зафиксир...</td>\n",
       "      <td>марсе происходить крупный землетрясение истори...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3395</th>\n",
       "      <td>Наука</td>\n",
       "      <td>Космос</td>\n",
       "      <td>американское европейский космический агентство...</td>\n",
       "      <td>nasa esa представлять ролик самый яркий момент...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3396 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       main_rubric        rubric  \\\n",
       "0     Происшествия  Происшествия   \n",
       "1     Происшествия  Происшествия   \n",
       "2     Происшествия  Происшествия   \n",
       "3     Происшествия  Происшествия   \n",
       "4     Происшествия  Происшествия   \n",
       "...            ...           ...   \n",
       "3391         Наука        Космос   \n",
       "3392         Наука        Космос   \n",
       "3393         Наука        Космос   \n",
       "3394         Наука        Космос   \n",
       "3395         Наука        Космос   \n",
       "\n",
       "                                              text_lemm  \\\n",
       "0     следствие попадать глава аналитический отдел п...   \n",
       "1     систематическое получение взятка арестовывать ...   \n",
       "2     место выезжать аварийный бригада сотрудник адм...   \n",
       "3     село старый хутор белгородский область попадат...   \n",
       "4     четверг декабрь обстрел вооруженный формирован...   \n",
       "...                                                 ...   \n",
       "3391  ученые томский университет система управление ...   \n",
       "3392  вес бортовой система космический аппарат число...   \n",
       "3393  двадцать пятый август год зарегистрировать мар...   \n",
       "3394  сейсмометр марсианский модуль insight зафиксир...   \n",
       "3395  американское европейский космический агентство...   \n",
       "\n",
       "                                             title_lemm  \n",
       "0     обвинение взяточничество арестовывать подмоско...  \n",
       "1     систематическое получение взятка арестовывать ...  \n",
       "2     приграничное село белгородский область попадат...  \n",
       "3     украинские войско обстреливать село белгородск...  \n",
       "4     губернатор гладков всу обстреливать село стары...  \n",
       "...                                                 ...  \n",
       "3391  томские ученый разрабатывать система электропи...  \n",
       "3392  томске разрабатывать система электропитание сп...  \n",
       "3393  ученые зафиксировать марсотрясение рекордный силы  \n",
       "3394  марсе происходить крупный землетрясение истори...  \n",
       "3395  nasa esa представлять ролик самый яркий момент...  \n",
       "\n",
       "[3396 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Для mac\n",
    "df_nlp = pd.read_csv(r'/Users/user/Documents/ML_Classic.csv')\n",
    "\n",
    "# Для win\n",
    "#df_nlp = pd.read_csv(r'C:\\Users\\User\\Downloads\\ML.csv'), если в начале лишних столбец с индексами\n",
    "\n",
    "df_ml = df_nlp.drop(df_nlp.columns[[0,3,4,5,6,7,8,9,10]], axis = 1)\n",
    "\n",
    "df_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728c446",
   "metadata": {},
   "source": [
    "### Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b2e6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "#from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032aa94",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e6e7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten, Input, GRU\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras import backend as K\n",
    "# from keras.engine.topology import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9137364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных тем:  9\n",
      "3396\n",
      "2864    нескольких месяц слух airpods max начинать пос...\n",
      "994     проекты реализовать нацпроект экология рамка р...\n",
      "464     великобритании призывать киев сильно бояться з...\n",
      "1740    правоохранительные орган испания задерживать л...\n",
      "1074    исполнительный директор альянс туристический а...\n",
      "                              ...                        \n",
      "1095    это повлиять неудобство новый ограничение стал...\n",
      "1130    ямале автоинспектор перекрывать дорога ради бе...\n",
      "1294    станислав  —  расти число российский называть ...\n",
      "860     женщина постоянно чувствовать боль жировик дав...\n",
      "3174    медики колумбийский университет устанавливать ...\n",
      "Name: text_lemm, Length: 2377, dtype: object\n",
      "291     официальный представитель кремль дмитрий песок...\n",
      "3056    например сквозной шифрование доступно больший ...\n",
      "2093    экс нападающий ряд футбольный клуб россия браз...\n",
      "432     всу продолжать терять командный состав донбасс...\n",
      "479     фоне близкий поставка американец зрк patriot у...\n",
      "                              ...                        \n",
      "1404    введения потолок цена частичный нефтяной эмбар...\n",
      "2258    ванкувер кэнакс серия буллит сламывать сопроти...\n",
      "3316    сибирском научно исследовательский институт ав...\n",
      "2114    декабря финал чемпионат мир футбол катар прохо...\n",
      "2279    данным источник спортсменка выигрывать грант м...\n",
      "Name: text_lemm, Length: 1019, dtype: object\n",
      "Максимальное количество слов в самом длинном описании заявки: 197 слов\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7, 3, 4, ..., 8, 3, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Темы\n",
    "df_topic = df_ml['main_rubric']\n",
    "\n",
    "\n",
    "# Разделение на выборки\n",
    "rubrics = ['Политика', 'Общество', 'Экономика', 'В мире', 'Спорт', 'Происшествия', 'Культура', 'Технологии', 'Наука']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "rubrics_list = df_ml['main_rubric'].to_list()\n",
    "rubric_labels = encoder.fit_transform(rubrics_list)\n",
    "\n",
    "X = df_ml['text_lemm']\n",
    "y = rubric_labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "my_tags = rubrics\n",
    "\n",
    "df_train = X_train\n",
    "df_test = X_test\n",
    "\n",
    "df = df_ml\n",
    "print(\"Количество уникальных тем: \", len(df['main_rubric'].unique()))\n",
    "print(len(df['text_lemm']))\n",
    "#print(df['text_lemm'].tolist())\n",
    "\n",
    "\n",
    "# Избавляемся от пустых строк\n",
    "df = df[df['text_lemm'].notna()]\n",
    "print(df_train)\n",
    "print(df_test)\n",
    "\n",
    "\n",
    "max_words = 0\n",
    "for desc in df_train:\n",
    "    words = len(desc.split())\n",
    "    if words > max_words:\n",
    "        max_words = words\n",
    "print('Максимальное количество слов в самом длинном описании заявки: {} слов'.format(max_words))\n",
    "\n",
    "\n",
    "# Максимальное количество слов\n",
    "num_words = 197\n",
    "\n",
    "# Количество классов (тем)\n",
    "nb_classes = len(df_topic.tolist())\n",
    "\n",
    "posts_train = df_train\n",
    "posts_test = df_test\n",
    "\n",
    "# Максимально количество слов в тексте\n",
    "max_post_len = 35\n",
    "\n",
    "\n",
    "# Преобразуем классы в векторный вид\n",
    "y_train\n",
    "#= to_categorical(df_train - 1, nb_classes)\n",
    "#y_test = to_categorical(df_test - 1, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2181a10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных тем:  9\n",
      "3396\n",
      "2864    нескольких месяц слух airpods max начинать пос...\n",
      "994     проекты реализовать нацпроект экология рамка р...\n",
      "464     великобритании призывать киев сильно бояться з...\n",
      "1740    правоохранительные орган испания задерживать л...\n",
      "1074    исполнительный директор альянс туристический а...\n",
      "                              ...                        \n",
      "1095    это повлиять неудобство новый ограничение стал...\n",
      "1130    ямале автоинспектор перекрывать дорога ради бе...\n",
      "1294    станислав  —  расти число российский называть ...\n",
      "860     женщина постоянно чувствовать боль жировик дав...\n",
      "3174    медики колумбийский университет устанавливать ...\n",
      "Name: text_lemm, Length: 2377, dtype: object\n",
      "291     официальный представитель кремль дмитрий песок...\n",
      "3056    например сквозной шифрование доступно больший ...\n",
      "2093    экс нападающий ряд футбольный клуб россия браз...\n",
      "432     всу продолжать терять командный состав донбасс...\n",
      "479     фоне близкий поставка американец зрк patriot у...\n",
      "                              ...                        \n",
      "1404    введения потолок цена частичный нефтяной эмбар...\n",
      "2258    ванкувер кэнакс серия буллит сламывать сопроти...\n",
      "3316    сибирском научно исследовательский институт ав...\n",
      "2114    декабря финал чемпионат мир футбол катар прохо...\n",
      "2279    данным источник спортсменка выигрывать грант м...\n",
      "Name: text_lemm, Length: 1019, dtype: object\n",
      "Максимальное количество слов в самом длинном описании заявки: 197 слов\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'нескольких месяц слух airpods max начинать поступать покупатель год назад устройство похоже считаться устаревать airpods pro второй поколение добавлять ряд технологический инновация который airpods m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f3/zqnx3rgj3m12khpr_cbwqlm80000gn/T/ipykernel_4319/3696044075.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Преобразуем классы в векторный вид\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    870\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    871\u001b[0m         \"\"\"\n\u001b[0;32m--> 872\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'нескольких месяц слух airpods max начинать поступать покупатель год назад устройство похоже считаться устаревать airpods pro второй поколение добавлять ряд технологический инновация который airpods m"
     ]
    }
   ],
   "source": [
    "# Темы\n",
    "df_topic = df_ml['main_rubric']\n",
    "\n",
    "\n",
    "# Разделение на выборки\n",
    "rubrics = ['Политика', 'Общество', 'Экономика', 'В мире', 'Спорт', 'Происшествия', 'Культура', 'Технологии', 'Наука']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "rubrics_list = df_ml['main_rubric'].to_list()\n",
    "rubric_labels = encoder.fit_transform(rubrics_list)\n",
    "\n",
    "X = df_ml['text_lemm']\n",
    "y = rubric_labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "my_tags = rubrics\n",
    "\n",
    "df_train = X_train\n",
    "df_test = X_test\n",
    "\n",
    "df = df_ml\n",
    "print(\"Количество уникальных тем: \", len(df['main_rubric'].unique()))\n",
    "print(len(df['text_lemm']))\n",
    "#print(df['text_lemm'].tolist())\n",
    "\n",
    "\n",
    "# Избавляемся от пустых строк\n",
    "df = df[df['text_lemm'].notna()]\n",
    "print(df_train)\n",
    "print(df_test)\n",
    "\n",
    "\n",
    "max_words = 0\n",
    "for desc in df_train:\n",
    "    words = len(desc.split())\n",
    "    if words > max_words:\n",
    "        max_words = words\n",
    "print('Максимальное количество слов в самом длинном описании заявки: {} слов'.format(max_words))\n",
    "\n",
    "\n",
    "# Максимальное количество слов\n",
    "num_words = 10000\n",
    "\n",
    "# Количество классов (тем)\n",
    "nb_classes = len(df_topic.tolist())\n",
    "\n",
    "posts_train = df_train\n",
    "posts_test = df_test\n",
    "\n",
    "# Максимально количество слов в тексте\n",
    "max_post_len = 35\n",
    "\n",
    "# Преобразуем классы в векторный вид\n",
    "y_train = to_categorical(df_train, nb_classes)\n",
    "y_test = to_categorical(df_test, nb_classes)\n",
    "\n",
    "# Производим токенизацию текста\n",
    "tokenizer = Tokenizer(num_words=num_words)  # 10000 самых встречаемых слов\n",
    "#tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(df['text_lemm'].tolist())\n",
    "print(len(tokenizer.index_word))\n",
    "#num_words = len(tokenizer.index_word) - 1\n",
    "\n",
    "# Слова в виде чисел\n",
    "sequences_train = tokenizer.texts_to_sequences(posts_train.tolist())\n",
    "sequences_test = tokenizer.texts_to_sequences(posts_test.tolist())\n",
    "\n",
    "# Преобразуем векторы к одной длине путем добавления нулей\n",
    "x_train = pad_sequences(sequences_train, maxlen=max_post_len)\n",
    "x_test = pad_sequences(sequences_test, maxlen=max_post_len)\n",
    "\n",
    "'''\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "tokens = []\n",
    "for text in df_train['text_lemm']:\n",
    "    token = text.split(' ')\n",
    "    token = [tok for tok in token if tok != \"\" and tok != '\\n']\n",
    "    tokens.append(token)\n",
    "\n",
    "df_train['tokens'] = tokens\n",
    "data = df_train['tokens']\n",
    "print(data)\n",
    "w2v_model = Word2Vec(data, min_count=1, size=100, window=5)\n",
    "\n",
    "embedding_index = dict()\n",
    "for word in w2v_model.wv.vocab:\n",
    "    embedding_index[word] = w2v_model.wv.word_vec(word)\n",
    "print(embedding_index)\n",
    "\n",
    "\n",
    "def prepare_matrix(embedding_dict, emb_size=100):\n",
    "    num_words = len(word_index)\n",
    "    embedding_matrix = np.zeros((num_words, emb_size))\n",
    "\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "\n",
    "        emb_vec = embedding_dict.get(word)\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[i] = emb_vec\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedding_matrix = prepare_matrix(embedding_index)\n",
    "num_words = len(word_index)\n",
    "'''\n",
    "\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "\n",
    "x_train = vectorize_sequences(x_train)\n",
    "x_test = vectorize_sequences(x_test)\n",
    "# print(x_train)\n",
    "\n",
    "#print(x_train)\n",
    "#print(tokenizer.word_index)\n",
    "#print(len(x_train))\n",
    "\n",
    "#smote = SMOTE(sampling_strategy='auto',\n",
    "#              random_state=0,\n",
    "#              k_neighbors=4)\n",
    "#X_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "#X_test, y_test = smote.fit_resample(x_test, y_test)\n",
    "#print(len(X_sm))\n",
    "#print(len(X_train))\n",
    "\n",
    "# Сеть LSTM ############################################################\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "'''\n",
    "model.add(Embedding(num_words, 256, input_length=max_post_len))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GRU(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(207, activation='softmax'))\n",
    "'''\n",
    "'''\n",
    "model.add(Embedding(num_words, 256, input_length=max_post_len))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(256, recurrent_dropout=0.5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(210, activation='softmax'))\n",
    "'''\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_shape=(10000,)))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(207, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "'''\n",
    "model.add(Embedding(num_words, 202, input_length=max_post_len))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(207, activation='softmax'))\n",
    "'''\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#model.load_weights('model1.h5')\n",
    "model_save_path = 'model.h5'\n",
    "checkpoint_callback = ModelCheckpoint(model_save_path,\n",
    "                                      monitor='val_accuracy',\n",
    "                                      save_best_only=True,\n",
    "                                      verbose=1)\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=7,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[checkpoint_callback])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "\n",
    "print()\n",
    "print(\"Оценка теста: {}\".format(score[0]))\n",
    "print(\"Оценка точности модели: {}\".format(score[1]))\n",
    "\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.xlabel('Эпоха обучения')\n",
    "plt.ylabel('Доля верных ответов')\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804fb400",
   "metadata": {},
   "source": [
    "### Содание нейронной сети без использования библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "INPUT_DIM = 1\n",
    "OUT_DIM = 9\n",
    "H_DIM = 5\n",
    "\n",
    "#dataset = [(df_ml['text_lemm'][i][None, ...], df_ml['rubric'][i]) for i in range(len(df_ml['rubric']))]\n",
    "\n",
    "W1 = np.random.rand(INPUT_DIM, H_DIM)\n",
    "b1 = np.random.rand(1, H_DIM)\n",
    "W2 = np.random.rand(H_DIM, OUT_DIM)\n",
    "b2 = np.random.rand(1, OUT_DIM)\n",
    "\n",
    "W1 = (W1 - 0.5) * 2 * np.sqrt(1/INPUT_DIM)\n",
    "b1 = (b1 - 0.5) * 2 * np.sqrt(1/INPUT_DIM)\n",
    "W2 = (W2 - 0.5) * 2 * np.sqrt(1/H_DIM)\n",
    "b2 = (b2 - 0.5) * 2 * np.sqrt(1/H_DIM)\n",
    "\n",
    "#Функция активации\n",
    "def relu(t):\n",
    "    return np.maximum(t,0)\n",
    "\n",
    "def softmax(t):\n",
    "    out = np.exp(t)\n",
    "    return out / np.sum(out)\n",
    "\n",
    "def softmax_batch(t):\n",
    "    out = np.exp(t)\n",
    "    return out / np.sum(out, axis=1, keepdims=True)\n",
    "\n",
    "def sparse_cross_entropy(z,y):\n",
    "    return -np.log(z[0,y])\n",
    "\n",
    "def sparse_cross_entropy_batch(z,y):\n",
    "    return -np.log(np.array([z[j, y[j]] for j in range(len(y))]))\n",
    "\n",
    "def to_full(y, num_classes):\n",
    "    y_full = np.zeros((1, num_classes))\n",
    "    y_full[0, y] = 1\n",
    "    return y_full\n",
    "\n",
    "def to_full_batch(y, num_classes):\n",
    "    y_full = np.zeros((len(y), num_classes))\n",
    "    for j, yj in enumerate(y):\n",
    "        y_full[j, yj] = 1\n",
    "    return y_full\n",
    "\n",
    "def relu_deriv(t):\n",
    "    return (t >= 0).astype(float)\n",
    "\n",
    "#Гиперпараметры нейросети\n",
    "#Скорость обучения (Learning rate)\n",
    "ALPHA = 0.001\n",
    "#Количество эпох\n",
    "NUM_EPOCHS = 100\n",
    "#Размер батча\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "loss_arr = []\n",
    "\n",
    "for ep in range(NUM_EPOCHS):\n",
    "    random.shuffle(dataset)\n",
    "    for i in range(len(X_train) // BATCH_SIZE):\n",
    "        #batch_x, batch_y = zip(*dataset[i*BATCH_SIZE : i*BATCH_SIZE+BATCH_SIZE])\n",
    "        #x = np.concatenate(batch_x, axis=0)\n",
    "        #y = np.array(batch_y)\n",
    "        \n",
    "        #Forward propagation (прямое распространение)\n",
    "        t1 = x @ W1 + b1\n",
    "        h1 = relu(t1)\n",
    "        t2 = h1 @ W2 + b2\n",
    "        z = softmax_batch(t2)\n",
    "        E = sparse_cross_entropy_batch(z,y)\n",
    "\n",
    "        #Back propagation (обратное распространение)\n",
    "        y_full = to_full_batch(y, OUT_DIM)\n",
    "        dE_dt2 = z - y_full\n",
    "        dE_dW2 = h1.T @ dE_dt2\n",
    "        dE_db2 = np.sum(dE_dt2, axis=0, keepdims=True)\n",
    "        dE_dh1 = dE_dt2 @ W2.T\n",
    "        dE_dt1 = dE_dh1 * relu_deriv(t1)\n",
    "        dE_dW1 = x.T @ dE_dt1\n",
    "        dE_db1 = np.sum(dE_dt1, axis=0, keepdims=True)\n",
    "\n",
    "        #Update (обновление весов)\n",
    "        W1 = W1 - ALPHA * dE_dW1\n",
    "        b1 = b1 - ALPHA * dE_db1\n",
    "        W2 = W2 - ALPHA * dE_dW2\n",
    "        b2 = b2 - ALPHA * dE_db2\n",
    "        W1 = W1 - ALPHA * dE_dW1\n",
    "        probs = predict(x)\n",
    "        pred_class = np.argmax(probs)\n",
    "        class_names = ['Политика', 'Общество', 'Экономика', 'В мире', 'Спорт', 'Происшествия', 'Культура', 'Технологии', 'Наука']\n",
    "\n",
    "        #Метрика \"зависимость ошибки от итерации\"\n",
    "        loss_arr.append(E)\n",
    "        \n",
    "        #Метрика \"точность предсказания\" ...\n",
    "        \n",
    "def predict(x):\n",
    "    t1 = x @ W1 + b1\n",
    "    h1 = relu(t1)\n",
    "    t2 = h1 @ W2 + b2\n",
    "    z = softmax_batch(t2)\n",
    "    return z\n",
    "\n",
    "def calc_accuracy():\n",
    "    correct = 0\n",
    "    for x,y in dataset:\n",
    "        z = predict(x)\n",
    "        y_pred = np.argmax(z)\n",
    "        if y_pred == y:\n",
    "            correct += 1\n",
    "    acc = correct / len(dataset)\n",
    "    return acc\n",
    "\n",
    "accuracu = calc_accuracy()\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "#График падения ошибки\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_arr)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
